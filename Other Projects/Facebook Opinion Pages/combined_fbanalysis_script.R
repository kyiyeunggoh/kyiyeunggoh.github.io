library(readr)
library(readxl)
library(zoo)
library(stringi)
library(dplyr)
library(readxl)
library(DT)
library(stringr)
library(pdftools)
library(qdapRegex)
library(maps)
library(SnowballC)
library(tm)
library(openxlsx)
library(quanteda)
library(tidytext)
library(qdapRegex)
library(lubridate)
library(SnowballC)
library(tm)
library(twitteR)
library(syuzhet)
library(plotly)
library(tidyverse)
library(gutenbergr)
library(tidytext)
library(stringr)
library(topicmodels)
library(tidyverse)
library(corrplot)
library(reshape2)
library(forecast)
library(sandwich)
library(ggplot2)
library(zoo)
library(lmtest)
library(car)
library(fUnitRoots)
library(stargazer)
library(kableExtra)
library(wordcloud)
library(purrrlyr)
library(syuzhet)
library(viridis)
library(hrbrthemes)
library(pheatmap)
library(ggthemes)

fbposts<-read_xlsx("combined_fb_posts.xlsx")

post_numbers<-fbposts%>%
  group_by(source) %>%
  summarise(source_count = length(source))%>%
  arrange(desc(source_count))

datatable(post_numbers)

atatable()TISG_FB<-fbposts%>%
  dplyr::filter(fbposts$source=="TISG")%>%
  mutate(mins=minute(TISG_FB$spectime))
## function (norm)

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

### function to get LDA
get_lda_cluster<-function(x,y){
  source_frame<-dplyr::filter(fbposts,fbposts$source==x)
  text_corpus <- VCorpus(VectorSource(source_frame$text))
  text_corpus_clean <- tm_map(text_corpus,
                              content_transformer(tolower))
  text_corpus_clean <- tm_map(text_corpus_clean, stemDocument)
  
  text_corpus_clean <- tm_map(text_corpus_clean, removeNumbers)
  text_corpus_clean <- tm_map(text_corpus_clean,
                              removeWords, stopwords())
  text_corpus_clean <- tm_map(text_corpus_clean, removePunctuation)
  
  text_corpus_clean <- tm_map(text_corpus_clean, stripWhitespace)
  
  text_dtm <- DocumentTermMatrix(text_corpus_clean)
  text_dtm
  findFreqTerms(text_dtm, lowfreq = 20)
  
  #remove zero entries
  ui = unique(text_dtm$i)
  text_dtm.new = text_dtm[ui,]
  
  
  text_lda <- LDA(text_dtm.new, k = y, method = "VEM", control = NULL)
  text_topics <- tidy(text_lda, matrix = "beta")
  
  text_top_terms <- text_topics %>%
    group_by(topic) %>%
    top_n(12, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)
  
  theme_update(plot.title = element_text(hjust = 0.5))
  p<-text_top_terms %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    ggtitle(paste0("Topic cluster generated by LDA of posts from ", x))+
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()
  
  png(paste("plot_", i, ".png", sep = ""), width=1100, height=800, res=120) # start export
  print(p) 
  dev.off() # finish export

}

sites<-unique(fbposts$source)

for (i in sites){
  get_lda_cluster(i,4)
  }

### LDA doesn't really tell us much besides the kinds of topics that they talk about

## WP - Pritam Singh ("Positive proportions across pages", "Negative", "Anger","Joy" and "Disgust")
### Sentiment of posts -PRITAM SINGH
find<-c("\\bpritam\\b","\\bpritam singh\\b") #find words
pritam_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  pritam_list[[i]] <- emocol
}
pritam_data = do.call(cbind, pritam_list)
emo_sum_pritam<-scale(pritam_data[1:7,1:11])
pheatmap(emo_sum_pritam, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Pritam Singh")


### Sentiment of posts - LEE HSIEN LOONG
find<-c("\\bpm lee\\b","\\blee hsien loong\\b","\\blhl\\b","\\bloong\\b") #find words
pm_leest = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  pm_leest[[i]] <- emocol
}
pmleest_data = do.call(cbind, pm_leest)
emo_sum_lhl<-scale(pmleest_data[-c(2,8,9,10),1:12])
pheatmap(emo_sum_lhl, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing PM Lee")


### Sentiment of posts - CHEE SOON JUAN
find<-c("\\bcsj\\b","\\bchee soon juan\\b") #find words
csj_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  csj_list[[i]] <- emocol
}
csj_data = do.call(cbind, csj_list)
emo_sum_csj<-scale(csj_data[-c(2,8,9,10),1:9])
pheatmap(emo_sum_csj, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Chee Soon Juan")

### Sentiment of posts - LIM TEAN
find<-c("\\btean\\b","\\blim tean\\b") #find words
limtean_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  limtean_list[[i]] <- emocol
}
limtean_data = do.call(cbind, limtean_list)
emo_sum_limtean<-scale(limtean_data[-c(2,8,9,10),1:10])
pheatmap(emo_sum_limtean, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Lim Tean")


### Sentiment of posts - TAN CHENG BOCK
find<-c("\\btcb\\b","\\bcheng bock\\b") #find words
tcb_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  tcb_list[[i]] <- emocol
}
tcb_data = do.call(cbind, tcb_list)
emo_sum_tcb<-scale(tcb_data[-c(2,8,9,10),1:10])
pheatmap(emo_sum_tcb, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Tan Cheng Bock")


### Sentiment of posts - STEVE CHIA
find<-c("\\bsteve chia\\b") #find words
steve_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  steve_list[[i]] <- emocol
}
steve_data = do.call(cbind, steve_list)
emo_sum_steve<-scale(steve_data[-c(2,8,9,10),-c(4)])
pheatmap(emo_sum_steve, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Steve Chia")


### Sentiment of posts - DESMOND LIM
find<-c("\\bdesmond lim\\b") #find words
desmond_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  desmond_list[[i]] <- emocol
}
desmond_data = do.call(cbind, desmond_list)
emo_sum_desmond<-scale(desmond_data[-c(2,8,9,10),-c(2)])
pheatmap(emo_sum_desmond, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Desmond Lim")

### Sentiment of posts - KENNETH J
find<-c("\\bkenneth jeyaretnam\\b") #find words
kenneth_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  kenneth_list[[i]] <- emocol
}
kenneth_data = do.call(cbind, kenneth_list)
emo_sum_kenneth<-scale(kenneth_data[-c(2,8,9,10),1:8])
pheatmap(emo_sum_kenneth, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Kenneth Jeyeratnam")


### Sentiment of posts - TAN JEE SAY
find<-c("\\btjs\\b","\\btan jee say\\b") #find words
tjs_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  tjs_list[[i]] <- emocol
}
tjs_data = do.call(cbind, tjs_list)
emo_sum_tjs<-scale(tjs_data[-c(2,8,9,10),1:7])
pheatmap(emo_sum_tjs, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Tan Jee Say")


### Sentiment of posts - GOH MENG SENG
find<-c("\\bmeng seng\\b","\\bgoh meng seng\\b") #find words
gms_list = list()
filter_frame<-dplyr::filter(fbposts, grepl(paste(find,collapse="|"), tolower(fbposts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  gms_list[[i]] <- emocol
}
gms_data = do.call(cbind, gms_list)
emo_sum_gms<-scale(gms_data[1:7,1:10])
pheatmap(emo_sum_gms, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Goh Meng Seng")

### Wordcloud of leaders
find<-c("\\bpritam\\b","\\bpritam singh\\b") #find words
cloud_frame<-dplyr::filter(fbposts,  grepl(paste(find,collapse="|"), tolower(text)))
sources<-unique(cloud_frame$source)
words <- unnest_tokens(cloud_frame, word, text)
words <- words %>% anti_join(stop_words)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=10, colors=brewer.pal(9, "Dark2"),random.order=F))
}


find<-c("\\bcsj\\b","\\bchee soon juan\\b") #find words
cloud_frame<-dplyr::filter(fbposts,  grepl(paste(find,collapse="|"), tolower(text)))
sources<-unique(cloud_frame$source)
words <- unnest_tokens(cloud_frame, word, text)
words <- words %>% anti_join(stop_words)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=10, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}

find<-c("\\bkenneth jeyaretnam\\b") #find words
cloud_frame<-dplyr::filter(fbposts,  grepl(paste(find,collapse="|"), tolower(text)))
sources<-unique(cloud_frame$source)
words <- unnest_tokens(cloud_frame, word, text)
words <- words %>% anti_join(stop_words)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=10, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}

### Sentiment of posts - TAN JEE SAY
find<-c("\\btjs\\b","\\btan jee say\\b") #find words
cloud_frame<-dplyr::filter(fbposts,  grepl(paste(find,collapse="|"), tolower(text)))
sources<-unique(cloud_frame$source)
words <- unnest_tokens(cloud_frame, word, text)
words <- words %>% anti_join(stop_words)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=10, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}

### Sentiment of posts - TAN CHENG BOCK
find<-c("\\btcb\\b","\\bcheng bock\\b") #find words
cloud_frame<-dplyr::filter(fbposts,  grepl(paste(find,collapse="|"), tolower(text)))
sources<-unique(cloud_frame$source)
words <- unnest_tokens(cloud_frame, word, text)
words <- words %>% anti_join(stop_words)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=10, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}

### Sentiment of posts - LEE HSIEN LOONG
find<-c("\\bpm lee\\b","\\blee hsien loong\\b","\\blhl\\b","\\bloong\\b") #find words
cloud_frame<-dplyr::filter(fbposts,  grepl(paste(find,collapse="|"), tolower(text)))
sources<-unique(cloud_frame$source)
words <- unnest_tokens(cloud_frame, word, text)
words <- words %>% anti_join(stop_words)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=10, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}


### Site activity
## Activity over time 
# cleaning dates 

fbposts$adDate<-str_extract(as.character(fbposts$time), "[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]")
fbposts$adDate<-as.Date(fbposts$adDate)
fbposts$adYear<-year(fbposts$adDate)

#some descriptive - across time volume

### Post date
date_posted<-fbposts%>%
  select(adDate)%>%
  group_by(adDate)

activity_date_ppap <- fbposts %>%
  dplyr::filter(adYear>2014)%>%
  group_by(source,adDate)%>%
  summarise(post_volume=log(length(adDate)))%>%
  na.omit()


ggplot(activity_date_ppap, aes(x=adDate,y=post_volume,group=source))+
  geom_line(aes(color=source))+
  ggtitle("Post activity since page inception")+
  theme_economist() 

activity_date_ppap %>%
  ggplot(aes(x=adDate,y=post_volume,group=source)) +
  geom_line(aes(color=source))+
  stat_smooth(method = "loess", formula = y ~ x, size = 1)+
  facet_wrap(~ source) +
  labs(title = "Post activity from 2015 onwards") +
  labs(x = "year", y = "Post\nVolume\n(logged)") +
  theme_economist()

#time posted - by hour, by maximum frequency, by clustering
### Post time
fbposts$spectime <- as.POSIXct(fbposts$time, 
                               format="%Y-%m-%d %H:%M:%S", 
                               tz="UTC")

fbposts$spectime <-with_tz(fbposts$spectime, "Asia/Hong_Kong")
fbposts$hour<-str_extract(as.character(fbposts$spectime), "[0-9][0-9]:[0-9][0-9]:[0-9][0-9]?")
fbposts$hour<-hour(fbposts$spectime)
fbposts$mins<-minute(fbposts$spectime)

activity_hour_ppap <- fbposts %>%
  group_by(source,hour)%>%
  summarise(post_volume=length(hour))%>%
  na.omit()%>%
  group_by(source) %>% 
  mutate(new = post_volume/max(post_volume))


ggplot(activity_hour_ppap, aes(x=hour,y=post_volume,group=source))+
  geom_line(aes(color=source))+
  scale_x_continuous(breaks = seq(0, 23, 1), labels = paste(seq(0, 23, 1), "00", sep = ":"))+
  ggtitle("Post activity over one day since page inception (by hour)")+
  theme_economist()+ 
  ylab("Number of posts")


activity_hour_ppap %>%
  ggplot(aes(x=hour,y=post_volume,group=source)) +
  geom_line(aes(color=source))+
  facet_wrap(~ source) +
  labs(title = "Post activity over one day since page inception (by hour)") +
  labs(x = "Hours", y = "Post\nVolume\n(raw)") +
  scale_x_continuous(breaks = seq(0, 23, 2), labels = paste(seq(0, 23, 2), "00", sep = ":"))+
  theme_economist()

activity_min_ppap <- fbposts %>%
  group_by(source,mins)%>%
  summarise(post_volume=length(mins))%>%
  na.omit()%>%
  group_by(source) %>% 
  mutate(min_norm = post_volume/max(post_volume))

activity_min_ppap %>%
  ggplot(aes(x=mins,y=min_norm,group=source)) +
  geom_line(aes(color=source))+
  facet_wrap(~ source) +
  labs(title = "Post activity over an hour since page inception (by minutes)") +
  labs(x = "Minutes", y = "Post\nVolume\n(normalised)") +
  scale_x_continuous(breaks = seq(0, 59, 10), labels = seq(0, 59, 10))+
  theme_economist()

### Typologies

### Pages that stop after 6pm - factcheckersg, factuallysg, GTS, everydaysg 

activity_hour_ppap %>%
  dplyr::filter(source=='DHRS'|source=='FBP'|source=='SNSG'|source=='TISG')%>%
  ggplot(aes(x=hour,y=new,group=source)) +
  geom_line(aes(color=source))+
  facet_wrap(~ source) +
  labs(title = "Post activity over one day since page inception (by hour)") +
  labs(x = "Hours", y = "Post\nVolume\n(normalised)") +
  scale_x_continuous(breaks = seq(0, 23, 2), labels = paste(seq(0, 23, 2), "00", sep = ":"))+
  theme_economist()

activity_min_ppap %>%
  dplyr::filter(source=='DHRS'|source=='FBP'|source=='SNSG'|source=='TISG')%>%
  ggplot(aes(x=mins,y=min_norm,group=source)) +
  geom_line(aes(color=source))+
  facet_wrap(~ source) +
  labs(title = "Post activity over an hour since page inception (by minutes)") +
  labs(x = "Minutes", y = "Post\nVolume\n(normalised)") +
  scale_x_continuous(breaks = seq(0, 59, 10), labels = seq(0, 59, 10))+
  theme_economist()

### Pages that suffer from "lunch hour dips" 

activity_hour_ppap %>%
  dplyr::filter(source=='FAP'|source=='TOC'|source=='WUSG'|source=='everydaysg')%>%
  ggplot(aes(x=hour,y=new,group=source)) +
  geom_line(aes(color=source))+
  facet_wrap(~ source) +
  labs(title = "Post activity over one day since page inception (by hour)") +
  labs(x = "Hours", y = "Post\nVolume\n(normalised)") +
  scale_x_continuous(breaks = seq(0, 23, 2), labels = paste(seq(0, 23, 2), "00", sep = ":"))+
  theme_economist()

### Pages that have irregular posting patterns- FAP, DHRS (almost like different timezone)
activity_hour_dif <- fbposts %>%
  group_by(source,hour)%>%
  dplyr::filter(source=='FBP'|source=='DHRS')%>%
  summarise(post_volume=length(hour))

ggplot(activity_hour_dif, aes(x=hour,y=post_volume,group=source))+
  geom_line(aes(color=source))+
  scale_x_continuous(breaks = seq(0, 23, 1), labels = paste(seq(0, 23, 1), "00", sep = ":"))+
  ggtitle("Pages with irregular posting hours")+
  theme_economist()+ 
  ylab("Number of posts")



### Pages that clearly schedule posts to maximise impact - TISG, Everydaysg



## Events based analysis
## Check which dates had the most volume - across all sites
activity_rate<-na.omit(activity_date_ppap)
top_activity_pp<-activity_rate%>%
  group_by(source) %>%
  top_n(n = 1)

## See the overlap of periods for all sites - for each sites top posts (more than 5 sites, sustained over 4 consecutive days that is non-Covid related)
top_activity_period<-activity_rate%>%
  group_by(source) %>%
  top_n(n = 100)

## 12 March 2016 to 8 May 2016 (Bukit Batok By-election, resignation of David Ong until day after election results)
## Murali Pillai, David Ong, Chee Soon Juan
batok_posts<-fbposts%>%
  dplyr::filter(adDate >= "2016-03-12", adDate <= "2016-05-08")

## activity during that period
bukitbatok_rate <- batok_posts%>%
  group_by(source,adDate)%>%
  summarise(post_volume=length(adDate))%>%
  na.omit()

ggplot(bukitbatok_rate, aes(x=adDate,y=post_volume,group=source))+
  geom_line(aes(color=source))+
  ggtitle("Page post activity during period of 2016 Bukit Batok SMC by-election") +
  theme_economist() +
  theme(plot.title = element_text(hjust = 0.5))+
  labs(x = "Date", y = "Number of posts") 


### Sentiment of posts - CHEE SOON JUAN
find<-c("\\bcsj\\b","\\bchee soon juan\\b") #find words
csj_list = list()
csj_filter<-dplyr::filter(batok_posts, grepl(paste(find,collapse="|"), tolower(batok_posts$text)))
sites<-unique(csj_filter$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  csj_list[[i]] <- emocol
}
csj_data = do.call(cbind, csj_list)
emo_sum_csj<-scale(csj_data[-c(2,8,9,10),1:6])
pheatmap(emo_sum_csj, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Chee Soon Juan")

sources<-unique(filter_frame$source)
words <- unnest_tokens(filter_frame, word, text)
words <- words %>% anti_join(stop_words)
words$word<- removeNumbers(words$word)

par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=1, colors=brewer.pal(8, "Dark2"),random.order=F,main="Title", title.size=2))
  }

  
### Sentiment of posts - DAVID ONG
find<-c("\\bdavid\\b","\\bwendy\\b") #find words
david_list = list()
filter_frame<-dplyr::filter(batok_posts, grepl(paste(find,collapse="|"), tolower(batok_posts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  david_list[[i]] <- emocol
}
david_data = do.call(cbind, david_list)
emo_sum_david<-scale(david_data[-c(2,8,9,10),1:5])
pheatmap(emo_sum_david, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing David Ong")

sources<-unique(filter_frame$source)
words <- unnest_tokens(filter_frame, word, text)
words <- words %>% anti_join(stop_words)
words$word<- removeNumbers(words$word)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=1, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}

### Sentiment of posts - MURALI
find<-c("\\bmurali \\b","\\bah mu\\b","\\bpillai \\b") #find words
murali_list = list()
filter_frame<-dplyr::filter(batok_posts, grepl(paste(find,collapse="|"), tolower(batok_posts$text)))
sites<-unique(filter_frame$source)

for (i in sites){
  filtered_frame<-dplyr::filter(filter_frame,filter_frame$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  murali_list[[i]] <- emocol
}
murali_data = do.call(cbind, murali_list)
emo_sum_murali<-scale(murali_data[-c(2,8,9,10),1:7])
pheatmap(emo_sum_murali, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Murali Pillai")

sources<-unique(filter_frame$source)
words <- unnest_tokens(filter_frame, word, text)
words <- words %>% anti_join(stop_words)
words$word<- removeNumbers(words$word)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=1, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}


### Trump Kim Summit
find<-c("\\bkim\\b","\\btrump\\b") #find words
summit_list = list()

trumpkimsummit_posts<-fbposts%>%
  dplyr::filter(adDate >= "2018-05-10", adDate <= "2018-06-26")%>%
  dplyr::filter(grepl(paste(find,collapse="|"), tolower(text)))

sites<-unique(trumpkimsummit_posts$source)

for (i in sites){
  filtered_frame<-dplyr::filter(trumpkimsummit_posts,trumpkimsummit_posts$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  summit_list[[i]] <- emocol
}
summit_data = do.call(cbind, summit_list)
emo_sum_summit<-scale(summit_data[-c(2,8,9,10),1:7])
pheatmap(emo_sum_summit, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Trump-Kim Summit")


words <- unnest_tokens(trumpkimsummit_posts, word, text)
words <- words %>% anti_join(stop_words)
words$word<- removeNumbers(words$word)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=3, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}


### Terrex incident - Nov 2016
find<-c("\\bterrex\\b") #find words
terrex_list = list()

terrex_posts<-fbposts%>%
  dplyr::filter(grepl(paste(find,collapse="|"), tolower(text)))

sites<-unique(terrex_posts$source)

for (i in sites){
  filtered_frame<-dplyr::filter(terrex_posts,terrex_posts$source==i)
  emo_baro <- get_nrc_sentiment(filtered_frame$text)
  emocol<-colSums(emo_baro)
  terrex_list[[i]] <- emocol
}
terrex_data = do.call(cbind, terrex_list)
emo_sum_terrex<-scale(terrex_data[-c(2,8,9,10),1:5])
pheatmap(emo_sum_terrex, treeheight_row = 0, treeheight_col = 0, main="Heatmap of sentiments when discussing Terrex incident")

words <- unnest_tokens(terrex_posts, word, text)
words <- words %>% anti_join(stop_words)
words$word<- removeNumbers(words$word)
par(mfrow=c(3,4))
for (i in sources) {
  words %>% 
    dplyr::filter(source == i) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words=40, min.freq=2, colors=brewer.pal(9, "Dark2"),random.order=F,main="Title",fixed.asp=TRUE,title.size = 3))
}


### Count posts

fbposts %>% 
  group_by(source) %>% 
  count()


### Reference counter - get 1 first then get all 12
wusg_posts<-fbposts%>%
  dplyr::filter(source=="WUSG")

WUSG_text<-paste(wusg_posts$text,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(WUSG_text, "\\bfabrications about the pap\\b"))
sum(str_count(WUSG_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(WUSG_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(WUSG_text, "\\bstates times\\b"))
sum(str_count(WUSG_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(WUSG_text, c("\\bsingapore matters\\b")))

toc_posts<-fbposts%>%
  dplyr::filter(source=="TOC")

toc_text<-paste(toc_posts$text,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(toc_text, "\\bfabrications about the pap\\b"))
sum(str_count(toc_text, c("\\bwake up singapore\\b")))
sum(str_count(toc_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(toc_text, "\\bstate news\\b"))
sum(str_count(toc_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(toc_text, c("\\bsingapore matters\\b")))
sum(str_count(toc_text, "\\bfactually singapore\\b"))
sum(str_count(toc_text, c("\\bfactchecker\\b","\\bfactchecker sg\\b")))
sum(str_count(toc_text, "\\bdemocracy and human rights\\b"))

tisg_posts<-fbposts%>%
  dplyr::filter(source=="TISG")

tisg_text<-paste(tisg_posts$text,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(tisg_text, "\\bfabrications about the pap\\b"))
sum(str_count(tisg_text, c("\\bwake up singapore\\b")))
sum(str_count(tisg_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(tisg_text, "\\bstate news\\b"))
sum(str_count(tisg_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(tisg_text, c("\\bsingapore matters\\b")))
sum(str_count(tisg_text, "\\bfactually singapore\\b"))
sum(str_count(tisg_text, c("\\bfactchecker\\b","\\bfactchecker sg\\b")))
sum(str_count(tisg_text, "\\bdemocracy and human rights\\b"))

everydaysg_posts<-fbposts%>%
  dplyr::filter(source=="everydaysg")

esg_text<-paste(everydaysg_posts$posttext,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(esg_text, "\\bfabrications about the pap\\b"))
sum(str_count(esg_text, c("\\bwake up singapore\\b")))
sum(str_count(esg_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(esg_text, "\\bstate news\\b"))
sum(str_count(esg_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(esg_text, c("\\bsingapore matters\\b")))

factsg_posts<-fbposts%>%
  dplyr::filter(source=="factuallysg")

factstext<-paste(factsg_posts$posttext,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(factstext, "\\bfabrications about the pap\\b"))
sum(str_count(factstext, c("\\bwakeup\\b")))
sum(str_count(factstext, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(factstext, "\\bstates times\\b"))
sum(str_count(factstext, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))

fap_posts<-fbposts%>%
  dplyr::filter(source=="FAP")

fap_text<-paste(fap_posts$posttext,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(fap_text, "\\bfabrications about the pap\\b"))
sum(str_count(fap_text, c("\\bwake up singapore\\b")))
sum(str_count(fap_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(fap_text, "\\bstate news\\b"))
sum(str_count(fap_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(fap_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(fap_text, c("\\bsingapore matters\\b")))

sgm_posts<-fbposts%>%
  dplyr::filter(source=="sgmatters")

sgm_text<-paste(sgm_posts$posttext,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(sgm_text, "\\bfabrications about the pap\\b"))
sum(str_count(sgm_text, c("\\bwake up singapore\\b")))
sum(str_count(sgm_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(sgm_text, "\\bstate news\\b"))
sum(str_count(sgm_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(sgm_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(sgm_text, "\\bfactually singapore\\b"))
sum(str_count(sgm_text, c("\\bfactchecker\\b","\\bfactchecker sg\\b")))

dhrs_posts<-fbposts%>%
  dplyr::filter(source=="DHRS")

dhrs_text<-paste(dhrs_posts$posttext,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(dhrs_text, "\\bfabrications about the pap\\b"))
sum(str_count(dhrs_text, c("\\bwake up singapore\\b")))
sum(str_count(dhrs_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(dhrs_text, "\\bstate news\\b"))
sum(str_count(dhrs_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(dhrs_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(dhrs_text, "\\bfactually singapore\\b"))
sum(str_count(dhrs_text, c("\\bfactchecker\\b","\\bfactchecker sg\\b")))

fsg_posts<-fbposts%>%
  dplyr::filter(source=="factuallysg")

fsg_text<-paste(fsg_posts$posttext,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(fsg_text, "\\bfabrications about the pap\\b"))
sum(str_count(fsg_text, c("\\bwake up singapore\\b")))
sum(str_count(fsg_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(fsg_text, "\\bstate news\\b"))
sum(str_count(fsg_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(fsg_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(fsg_text, c("\\bfactchecker\\b","\\bfactchecker sg\\b")))

factsg_posts<-fbposts%>%
  dplyr::filter(source=="factcheckersg")

factsg_text<-paste(factsg_posts$posttext,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(factsg_text, "\\bfabrications about the pap\\b"))
sum(str_count(factsg_text, c("\\bwake up singapore\\b")))
sum(str_count(factsg_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(factsg_text, "\\bstate news\\b"))
sum(str_count(factsg_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(factsg_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(factsg_text, "\\bfactually singapore\\b"))

snsg_posts<-fbposts%>%
  dplyr::filter(source=="snsg")

snsg_text<-paste(snsg_posts$posttext,collapse=' ')%>%
  tolower()%>%
  str_replace_all(., "[\r\n]" , " ")%>%
  gsub('[[:punct:] ]+',' ',.)

sum(str_count(snsg_text, "\\bfabrications about the pap\\b"))
sum(str_count(snsg_text, c("\\bwake up singapore\\b")))
sum(str_count(snsg_text, c("\\btoc\\b","theonlinecitizencom","\\btheonlinecitizen\\b","\\bthe online citizen\\b")))
sum(str_count(snsg_text, "\\bstate news\\b"))
sum(str_count(snsg_text, c("\\bthe independent\\b","\\btheindependent\\b","\\bindependentsg\\b")))
sum(str_count(snsg_text, c("\\beveryday singapore\\b","\\beveryday sg\\b","\\beverydaysg\\b")))
sum(str_count(snsg_text, "\\bfactually singapore\\b"))
sum(str_count(snsg_text, c("\\bfactchecker\\b","\\bfactchecker sg\\b")))
